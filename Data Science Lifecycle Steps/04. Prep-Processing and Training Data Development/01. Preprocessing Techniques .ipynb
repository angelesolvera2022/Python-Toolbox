{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa788349-be29-46d9-98c6-aeb6949bbd24",
   "metadata": {},
   "source": [
    "1. Problem identification \n",
    "\n",
    "2. Data wrangling\n",
    "\n",
    "3. Exploratory data analysis\n",
    "\n",
    "4. **Prep-processing and training data development**\n",
    "\n",
    "5. Modeling (Machine learning steps)\n",
    "\n",
    "6. Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a53bbc-6771-469b-91b4-f97d41a3d908",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-success\">\n",
    "<h3>Feature Engineering</h3>\n",
    "\n",
    "- The process of transforming raw data into meaningful features that improve the performance of machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a876137-2c2e-4215-95c3-857004597fc5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**<span style=\"background-color: lightpink;\"> Numerical Feature Transformations</span>**\n",
    "\n",
    "**<font color='seagreen'><b>Binning</b></font>**\n",
    "\n",
    "- Convert continuous variables into categorical bins (e.g., age groups).\n",
    "\n",
    "**<font color='seagreen'><b>Scaling</b></font>**\n",
    "\n",
    "- changing the values of features values so they are all in the same scale. \n",
    "\n",
    "        - Standarding: Use when you want to preserve outliers and assume a normal distribution.\n",
    "                  - Centers data around mean = 0 and scales by standard deviation\n",
    "  \n",
    "        - Normalizing: Use when you want all features in a bounded range, especially for neural networks.\n",
    "                  - Rescales data to a fixed range, usually 0 to 1\n",
    "  \n",
    "\n",
    "**<font color='seagreen'><b>Log Transformation</b></font>**\n",
    "\n",
    "- Reduce skewness in highly skewed distributions.\n",
    "\n",
    "**<font color='seagreen'><b>Polynomial Features</b></font>**\n",
    "\n",
    "- Add squared or interaction terms to capture non-linear relationships.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895e044b-b6d3-4426-be7d-eaacaf52797f",
   "metadata": {},
   "source": [
    "**<span style=\"background-color: lightpink;\">Categorical Feature Transformations</span>**\n",
    "\n",
    "**<font color='seagreen'><b>One-Hot Encoding</b></font>**\n",
    "\n",
    "- Converts categorical values into binary columns—each category becomes its own column with a 1 or 0 indicating presence.\n",
    "\n",
    "**<font color='seagreen'><b>Label Encoding</b></font>**\n",
    "\n",
    "- Assign numeric labels to categories (use cautiously). Red → 0, Blue → 1, Green → 2\n",
    "\n",
    "\n",
    "**<font color='seagreen'><b>Frequency Encoding</b></font>**\n",
    "\n",
    "- Replace categories with their frequency counts.\n",
    "\n",
    "**<font color='seagreen'><b>Target Encoding</b></font>**\n",
    "\n",
    "- Replace categories with the mean of the target variable (risk of leakage!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4440dbaf-b04b-4bd6-8696-56b766abd63f",
   "metadata": {},
   "source": [
    "**<span style=\"background-color: lightpink;\">Dimensionality Reduction------------------</span>**\n",
    "\n",
    "- The process of reducing the number of input features in a dataset while preseving as uch relevant information as possible.\n",
    "\n",
    "It helps:\n",
    "\n",
    "- Simplify models\n",
    "- Reduce noise and overfitting\n",
    "- Improve computational efficiency\n",
    "- Visualize high dimensional data\n",
    "\n",
    "**<font color='seagreen'><b>Dimensionality Reduction Techniques</b></font>**\n",
    "\n",
    "- PCA (Principal Component Analysis): Projects data onto directions of maximum variance.\n",
    "\n",
    "- t-SNE / UMAP: Non-linear methods for visualizing complex patterns in 2D or 3D.\n",
    "\n",
    "- Feature Selection: Keeps only the most informative features based on statistical or model-based criteria.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ccc8d6-0570-4d4a-ae35-d4d3fbc7901c",
   "metadata": {},
   "source": [
    "**<span style=\"background-color: lightpink;\">Splitting Data--------------------------------</span>**\n",
    "\n",
    "- Dividing your data into a training set and test set so you can build and evaluate machine learning models fairly. \n",
    "\n",
    "\n",
    "**<font color='seagreen'><b>Splitting Techniques</b></font>**\n",
    "\n",
    "- **Train- Test Split**: Divide your data into two parts: one to train the model, and one to test how well it performs.\n",
    "  \n",
    "- **K-Fold Cross Validation**: Split the data into k equal parts (folds). Train on k–1 folds and test on the remaining one. Repeat k times to get a more reliable performance estimate\n",
    "  \n",
    "- **Stratified Sampling**: Split the data while keeping the same proportion of classes (e.g., 70% cats, 30% dogs) in each subset.\n",
    "  \n",
    "              - In stratified sampling, each class doesn’t need to have the same number of samples, but rather the same proportion in each split.\n",
    "              - Let’s say your dataset has:\n",
    "\n",
    "                                80% Class A\n",
    "                                \n",
    "                                20% Class B\n",
    "                                \n",
    "                                If you use stratified sampling with a 70/30 train-test split:\n",
    "                                \n",
    "                                Your training set will have ~80% Class A and ~20% Class B\n",
    "                                \n",
    "                                Your test set will also have ~80% Class A and ~20% Class B\n",
    "\n",
    "\n",
    "  \n",
    "- **Random Permutation**: Shuffle the data randomly before splitting, to avoid any order bias. Often done with np.random.permutation() in NumPy.\n",
    "\n",
    "             - Use when the dataset has no ID column.\n",
    "\n",
    "- **crc32 identifier**: A checksum function that turns a string (like an ID) into a number. Used to split data consistently by hashing IDs, especially when you want reproducible splits without leaking data\n",
    "\n",
    "              - Use when the dataset has an ID column.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
