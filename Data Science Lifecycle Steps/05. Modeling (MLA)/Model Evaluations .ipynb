{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11514d53-cc06-4b67-8db4-18d8469d6076",
   "metadata": {},
   "source": [
    "1. Problem identification \n",
    "\n",
    "2. Data wrangling\n",
    "\n",
    "3. Exploratory data analysis\n",
    "\n",
    "4. Prep-processing and training data development\n",
    "\n",
    "5. **Modeling (Machine learning steps)**\n",
    "\n",
    "6. Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8dc006-5cd3-490a-8341-1b352e3b2280",
   "metadata": {},
   "source": [
    "# Measures of Godness of Fit "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddb5074-e82a-4e3c-8600-06844be9b5b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div class=\"span5 alert alert-warning\">\n",
    "<h3>Regression Metrics</h3>\n",
    "\n",
    "- https://web.archive.org/web/20240117214702/https://www.kdnuggets.com/2018/04/right-metric-evaluating-machine-learning-models-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c688dcb-0d0b-42f0-9978-0b1c9ee5a68a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <font color='magenta'><b>MAE - Mean Absolute Error </b></font> \n",
    "\n",
    "Definition: Is calculated by first finding the absolute differences between each predicted value and its corresponding actual value, then averaging those differences. ‚à£Actual ùëñ ‚àí Predicted ùëñ ‚à£. Its only 1 number. \n",
    "- you want your mae to be low \n",
    "- This is more useful that R^2 evaluation method when you care more about how \"close\" your predictions are to the true values, in other words the difference between predicted and actual values rather than the variability.\n",
    "- A linear metric‚Äîeach error contributes equally to the final score.\n",
    "- Example: Predicting inventory levels or demand for products. Businesses use MAE to ensure their predictions are accurate enough to prevent stockouts or overstock situations.\n",
    "\n",
    "**Mathematical Steps** \n",
    "\n",
    "1Ô∏è‚É£ Find the error\n",
    "\n",
    "Subtract each predicted value from its actual value:\n",
    "\n",
    "$$ \\text{Error} = y_{\\text{actual}} - y_{\\text{predicted}} $$\n",
    "\n",
    "\n",
    "2Ô∏è‚É£ Take the absolute value\n",
    "\n",
    "Convert all errors to positive values (so negative errors don‚Äôt cancel out positive errors):\n",
    "\n",
    "$$ \\text{Absolute Error} = |y_{\\text{actual}} - y_{\\text{predicted}}| $$\n",
    "\n",
    "\n",
    "3Ô∏è‚É£ Find the average of all absolute errors\n",
    "\n",
    "Sum up all absolute errors and divide by the total number of observations \n",
    "ùëõ :\n",
    "\n",
    "$$ MAE = \\frac{1}{n} \\sum |y_{\\text{actual}} - y_{\\text{predicted}}| $$\n",
    "\n",
    "\n",
    "**<u>Function Option <u>**\n",
    "\n",
    "**1) Create a function that calculates the meanof the absolute errors**\n",
    "- you can calculate this for both the training data and test data.\n",
    "- order always matter its always actual first predicted 2nd when calling both options\n",
    "\n",
    "\n",
    "`def mae(y, ypred):`\n",
    "\n",
    "    abs_error = np.abs(y - ypred)\n",
    "    mae = np.mean(abs_error)\n",
    "    return mae\n",
    "  \n",
    "`mae(actual_values, predictions)`\n",
    "\n",
    "**<u> sk.learn option <u>**\n",
    "\n",
    "`from sklearn.metrics import mean_absolute_error`\n",
    "\n",
    "`train_mae = mean_absolute_error(train_actual_values, train_predicted_values)`\n",
    "\n",
    "`test_mae = mean_absolute_error(test_actual_values, test_predicted_values)`\n",
    "\n",
    "\n",
    "\n",
    "-  this essentially tells you that, on average, you might expect to be off by around __ (the output of function) if you guessed based on average of actual numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabcf1ee-0e50-4d2c-bd2c-3b9798e53684",
   "metadata": {},
   "source": [
    "## <font color='magenta'><b>Mean Absolute Percentage Error (MAPE)</b></font> \n",
    "\n",
    "Definition: MAPE (Mean Absolute Percentage Error) measures the average percentage difference between predicted and actual values in a regression model. It evaluates prediction accuracy in relative terms, making it useful for comparing performance across datasets of different scales.\n",
    "\n",
    "- Lower MAPE indicates better model accuracy.\n",
    "- Difference from MAE: While MAE provides error in absolute units, MAPE expresses error as a percentage, making it more interpretable when working with varying scales.\n",
    "\n",
    "\n",
    "**Mathematical Steps** \n",
    "\n",
    "1Ô∏è‚É£ Find the absolute errors\n",
    "\n",
    "Subtract the actual values from the predicted values:\n",
    "\n",
    "$$ \\text{Error} = |y_{\\text{actual}} - y_{\\text{predicted}}| $$\n",
    "\n",
    "\n",
    "2Ô∏è‚É£ Convert each error into a percentage\n",
    "\n",
    "Divide each absolute error by its actual value to get a percentage error:\n",
    "\n",
    "$$ \\text{Percentage Error} = \\left( \\frac{|y_{\\text{actual}} - y_{\\text{predicted}}|}{y_{\\text{actual}}} \\right) \\times 100 $$\n",
    "\n",
    "3Ô∏è‚É£ Calculate the average percentage error\n",
    "\n",
    "Sum all percentage errors and divide by the total number of observations (n):\n",
    "\n",
    "$$ MAPE = \\frac{1}{n} \\sum \\left( \\frac{|y_{\\text{actual}} - y_{\\text{predicted}}|}{y_{\\text{actual}}} \\times 100 \\right) $$\n",
    "\n",
    "\n",
    "\n",
    "**Metric** \n",
    "\n",
    "`from sklearn.metrics import mean_absolute_percentage_error`\n",
    "\n",
    "`mape_score = mean_absolute_percentage_error(actual_values, predicted_values) * 100`\n",
    "\n",
    "`print(f\"MAPE: {mape_score:.2f}%\")`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0e97fe",
   "metadata": {},
   "source": [
    "## <font color='magenta'><b>RMSE- Root Mean Squared Error</b></font> \n",
    "\n",
    "Evaluating the model using \n",
    "(RMSE) is a metric used to evaluate the accuracy of a regression model. It measures the average magnitude of the errors between the predicted values and the actual values. In other words, it tells you how well your model's predictions match the actual data. Ideally a low number is good. That will demostrate your model is accurate. \n",
    "- Tends to be the default metric\n",
    "- Represents the sample standard deviation of prediction errors (residuals).\n",
    "- Penalizes large errors more than Mean Absolute Error (MAE).\n",
    "- Typically larger or equal to MAE, unless all prediction errors are identical.\n",
    "- Scores always need to be possitive\n",
    "\n",
    "**Mathematical Steps** \n",
    "\n",
    "1Ô∏è‚É£ Find the errors\n",
    "\n",
    "Subtract each predicted value from its actual value:\n",
    "\n",
    "$$ \\text{Error} = y_{\\text{actual}} - y_{\\text{predicted}} $$\n",
    "\n",
    "2Ô∏è‚É£ Square each error\n",
    "\n",
    "This ensures larger errors are penalized more and turns negative values into possitive values. \n",
    "\n",
    "$$ \\text{Squared Error} = (y_{\\text{actual}} - y_{\\text{predicted}})^2 $$\n",
    "\n",
    "\n",
    "3Ô∏è‚É£ Find the Mean Squared Error (MSE)\n",
    "\n",
    "Sum all squared errors and divide by the total number of observations \n",
    "ùëõ:\n",
    "\n",
    "$$ MSE = \\frac{1}{n} \\sum (y_{\\text{actual}} - y_{\\text{predicted}})^2 $$\n",
    "\n",
    "4Ô∏è‚É£ Take the square root of MSE\n",
    "\n",
    "This converts the squared error back to the original units of the data:\n",
    "\n",
    "$$ RMSE = \\sqrt{MSE} $$\n",
    "\n",
    "\n",
    "**Metric**\n",
    "  \n",
    "`from sklearn.metrics import mean_squared_error`\n",
    "\n",
    "`lin_rmse = mean_squared_error (y,life_expc_pred, squared = False)`\n",
    "\n",
    "`lin_rmse`\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5441da15-aa66-41c8-bda0-1e291f354328",
   "metadata": {},
   "source": [
    "## <font color='magenta'><b>R¬≤ (coefficient of determination)</b></font> \n",
    "\n",
    "-  Measures the part of the variance (spread) in the dependent variable (y variable - the value that was predicted) that is explained by the independent variables (existing variables used to predict the y variables) in the model.\n",
    "It compares the predicted values to the actual values to determine how much of the variability in y is explained by the model.\n",
    "-  In simple terms, it tells you how well your model's predictions match the actual data, with a value between 0 and 1. A higher R¬≤ value indicates a better fit\n",
    "\n",
    "**Mathematical Steps** \n",
    "\n",
    "1Ô∏è‚É£ Find the mean of actual values\n",
    "\n",
    "Calculate the average of all actual values:\n",
    "\n",
    "$$ \\bar{y} = \\frac{1}{n} \\sum y_{\\text{actual}} $$\n",
    "\n",
    "2Ô∏è‚É£ Compute the Total Sum of Squares (TSS)\n",
    "\n",
    "Measure the total variation in actual values:\n",
    "\n",
    "$$ TSS = \\sum (y_{\\text{actual}} - \\bar{y})^2 $$\n",
    "\n",
    "3Ô∏è‚É£ Compute the Residual Sum of Squares (RSS)\n",
    "\n",
    "Measure how much error remains after predictions:\n",
    "\n",
    "$$ RSS = \\sum (y_{\\text{actual}} - y_{\\text{predicted}})^2 $$\n",
    "\n",
    "4Ô∏è‚É£ Calculate R¬≤\n",
    "\n",
    "Determine the proportion of variance explained by the model:\n",
    "\n",
    "$$ R¬≤ = 1 - \\frac{RSS}{TSS} $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**<u>Function Option <u>**\n",
    "\n",
    "**1) Create a new variable with the data not containing any categorical data just numerical**\n",
    "\n",
    "**2) split data training set, test set, training target values and test target values**\n",
    "\n",
    "- x : should have all input features (all the columns except the target values)\n",
    "- y : should have all the target values (used to predict)\n",
    "   \n",
    " `X_train, X_test, y_train, y_test = train_test_split(data.drop(columns='target_values'),\n",
    "data.target_values, test_size=0.3, \n",
    "   random_state=47)`\n",
    "\n",
    "\n",
    "**3) create a function that calculates R^2** of either training data and test data. Will calculate depending what inouts you give training or test data. \n",
    " \n",
    "- ```\n",
    "    def r_squared(y, ypred):\n",
    "\n",
    "    ybar = np.sum(y) / len(y) #yes, we could use np.mean(y)\n",
    "    sum_sq_tot = np.sum((y - ybar)**2) #total sum of squares error\n",
    "    sum_sq_res = np.sum((y - ypred)**2) #residual sum of squares error\n",
    "    R2 = 1.0 - sum_sq_res / sum_sq_tot\n",
    "    return R2\n",
    "  ```\n",
    "  \n",
    "**4) Make predictions using the mean of either the training data or test data times an array of ones the lenght of the training data or test data**\n",
    "\n",
    "- `baseline_predictions = mean_value * np.ones(len(actual_values))\n",
    "baseline_predictions[:5]`\n",
    "\n",
    " \n",
    "    or use  `sklearn` dummy regressor that passes trough the training data\n",
    "\n",
    "- `predictions = model.predict(input_features)\n",
    "predictions[:5]]` \n",
    "\n",
    "**5) call the function, passing trough the actual data and the predictions**\n",
    "\n",
    "- `r_squared(actual_values, predictions)`\n",
    "\n",
    "\n",
    "**Metric**\n",
    "\n",
    "\n",
    "**<u> sk.learn option <u>**\n",
    "\n",
    "` from sklearn.metrics import r2_score`\n",
    "\n",
    "`train_r2 = r2_score(train_actual_values, train_predicted_values)`\n",
    "\n",
    "`test_r2= r2_score(test_actual_values, test_predicted_values)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae5586d-0ac9-43f8-950b-3ba7a56000cd",
   "metadata": {},
   "source": [
    "## <font color='magenta'><b>Adjusted R¬≤</b></font> \n",
    "\n",
    "\n",
    "MEtric that shows how well a regression model explains the variability in the data, but with a correction for the number of predictors in the model. Unlike regular R¬≤, which increases as more predictors are added, Adjusted R¬≤ only increases if the new predictors actually add value. If they don‚Äôt, it decreases to prevent overfitting. Think of it as a smarter version of R¬≤‚Äîhelping you assess if adding more variables truly improves your model or just makes it unnecessarily complex.\n",
    "\n",
    "**Mathematical Steps** \n",
    "\n",
    "1Ô∏è‚É£ Calculate R¬≤\n",
    "\n",
    "Use the standard coefficient of determination formula:\n",
    "\n",
    "$$ R¬≤ = 1 - \\frac{RSS}{TSS} $$\n",
    "\n",
    "2Ô∏è‚É£ Adjust for number of predictors\n",
    "\n",
    "Modify R¬≤ to account for model complexity and prevent overestimation:\n",
    "\n",
    "$$ R¬≤_{\\text{adj}} = 1 - \\left( \\frac{(1 - R¬≤)(n - 1)}{n - k - 1} \\right) $$\n",
    "\n",
    "\n",
    "**Metric** \n",
    "\n",
    "`from sklearn.linear_model import LinearRegression`\n",
    "\n",
    "`from sklearn.metrics import r2_score`\n",
    "\n",
    "Fit the model\n",
    "\n",
    "`model = LinearRegression()`\n",
    "\n",
    "`model.fit(X_train, y_train)`\n",
    "\n",
    "Predict values\n",
    "\n",
    "`y_pred = model.predict(X_test)`\n",
    "\n",
    "`r2 = r2_score(y_test, y_pred)`\n",
    "\n",
    "`n = X_test.shape[0]  # Number of observations`\n",
    "\n",
    "`k = X_test.shape[1]  # Number of features`\n",
    "\n",
    "`adjusted_r2 = 1 - ((1 - r2) * (n - 1) / (n - k - 1))`\n",
    "\n",
    "`print(\"Adjusted R¬≤:\", adjusted_r2)`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d6dc4c-8dae-4761-9dc3-c7f8be8ebdb9",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-warning\">\n",
    "<h3>Other Regression Metrics</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ded0060-d3a4-4e8c-8837-30ff068e2e59",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### <font color='magenta'><b>F-test - linear regression \n",
    "</b></font> \n",
    "\n",
    "Purpose: Evaluates the overall significance of the linear regression model. It tests whether at least one of the predictors is significantly related to the dependent variable.\n",
    "\n",
    "Hypothesis:\n",
    "\n",
    "Null Hypothesis (\n",
    "ùêª\n",
    "0\n",
    "): All regression coefficients are equal to zero (no effect).\n",
    "\n",
    "Alternative Hypothesis (\n",
    "ùêª\n",
    "1\n",
    "): At least one regression coefficient is not equal to zero.\n",
    "\n",
    "### <font color='magenta'><b> t-test for Regression Coefficients: - linear regression </b></font> \n",
    "\n",
    "Purpose: Tests the significance of individual regression coefficients, determining whether each predictor has a significant effect on the dependent variable.\n",
    "\n",
    "Hypothesis:\n",
    "\n",
    "Null Hypothesis (\n",
    "ùêª\n",
    "0\n",
    "): The coefficient is equal to zero (no effect).\n",
    "\n",
    "Alternative Hypothesis (\n",
    "ùêª\n",
    "1\n",
    "): The coefficient is not equal to zero.\n",
    "\n",
    "### <font color='magenta'><b>Residual Analysis- linear regression</b></font> \n",
    "\n",
    "Purpose: Analyzes the residuals (differences between observed and predicted values) to check for patterns that might indicate issues with the model, such as non-linearity, heteroscedasticity, or outliers.\n",
    "\n",
    "Plots:\n",
    "\n",
    "Residual vs. Fitted Plot: Checks for non-linearity and equal variance.\n",
    "\n",
    "Q-Q Plot: Assesses the normality of residuals.\n",
    "\n",
    "### <font color='magenta'><b>Durbin-Watson Test - linear regression </b></font> \n",
    "\n",
    "Purpose: Tests for the presence of autocorrelation (correlation of residuals) in the residuals from a linear regression model.\n",
    "\n",
    "Value Range:\n",
    "\n",
    "2 indicates no autocorrelation.\n",
    "\n",
    "Values < 2 indicate positive autocorrelation.\n",
    "\n",
    "Values > 2 indicate negative autocorrelation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa95c36c-41d3-4867-b5ea-6a1a3f1c9dff",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799285ee-2e85-4941-8a52-1589290008ca",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-warning\">\n",
    "<h3>Classification Metrics</h3>\n",
    "\n",
    "- https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-machine-learning-tips-and-tricks\n",
    "- https://web.archive.org/web/20240103185927/https://www.kdnuggets.com/2018/06/right-metric-evaluating-machine-learning-models-2.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7703c58c",
   "metadata": {},
   "source": [
    "### <font color='coral'><b>Confusion Matrix - classification models </b></font>\n",
    "\n",
    "### <font color='coral'><b>Binary Classification</b></font>\n",
    "\n",
    "\n",
    "<span style=\"background-color: peachpuff;\">Table that represents which labels where correctly predicted and which were not.</span>\n",
    " It compares the actual labels (true values) with the predicted labels from the model, helping to identify errors.\n",
    "\n",
    "- Each column represents the predicted class.\n",
    "- The diagonal elements represent the correctly classified images.\n",
    "- The off-diagonal elements represent the misclassified images.\n",
    "  \n",
    "1Ô∏è‚É£ True Positives (TP) ‚Äì Correctly predicted positive cases.\n",
    "- Recall, sensitivity\n",
    "\n",
    "2Ô∏è‚É£ True Negatives (TN) ‚Äì Correctly predicted negative cases.\n",
    "- Specificity \n",
    "\n",
    "3Ô∏è‚É£ False Positives (FP) ‚Äì Incorrectly predicted as positive (Type I error). \n",
    "- \t1-specificity\n",
    "\n",
    "4Ô∏è‚É£ False Negatives (FN) ‚Äì Incorrectly predicted as negative (Type II error).\n",
    "- Miss Rate \n",
    "\n",
    "**Model evaluation**\n",
    "\n",
    "`from sklearn.metrics import confusion_matrix`\n",
    "\n",
    "`cm = confusion_matrix (y_test,y_pred)`\n",
    "\n",
    "**Vizualise the confusion matrix**\n",
    "\n",
    "`from sklearn.metrics import ConfusionMatrixDisplay`\n",
    "\n",
    "\n",
    "`_, ax = plt.subplots()`\n",
    "\n",
    "`display_cm = ConfusionMatrixDisplay(confusion_matrix = cm, \n",
    "                                    display_labels = ['not target variable', 'target variable'])`\n",
    "                                    \n",
    "`ax.set_xticks([0, 1])`\n",
    "\n",
    "`ax.set_yticks([0, 1])`\n",
    "\n",
    "`ax.set_xticklabels(labels = ['no target variable', 'target variable'], fontsize = 8)`\n",
    "\n",
    "`ax.set_yticklabels(labels = ['no target variable', 'target variable'], fontsize = 8)`\n",
    "\n",
    "`display_cm.plot(ax = ax)`\n",
    "\n",
    "`plt.show()`\n",
    "\n",
    "             \n",
    "         \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61ba9c2-f4da-4ba6-a422-980decf28586",
   "metadata": {},
   "source": [
    "<img src=\"confusionmatrix.png\" alt=\"Confusion Matrix\" style=\"width:800px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7066598f",
   "metadata": {},
   "source": [
    "### <font color='coral'><b>Precision - classification models</b></font>\n",
    "\n",
    "- when I say something is possitive how often Im I right?\n",
    "- key performance metric tells<span style=\"background-color: peachpuff;\"> you how many predicted positive cases were .</span>\n",
    "\n",
    "\n",
    "<span style=\"background-color: peachpuff;\">Precision = True¬†Positives /True¬†Positives + False¬†Positives</span>\n",
    "\n",
    "Important when false positives are costly (e.g., spam filtering).\n",
    "\n",
    " * High Precision = model has low false positive rate meaning its good at accurately predicting\n",
    " * Low Precision: model has high false possitive rate meaning model is not good at predicting \n",
    "\n",
    "**Metric** \n",
    "\n",
    "`from sklearn.metrics import precision_score`\n",
    "\n",
    "`# Example: Actual vs. Predicted labels`\n",
    "\n",
    "`y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 1]  # Actual labels`\n",
    "\n",
    "`y_pred = [1, 0, 1, 0, 0, 1, 1, 0, 1, 1]  # Predicted labels`\n",
    "\n",
    "`# Calculate Precision`\n",
    "\n",
    "`precision = precision_score(y_true, y_pred)`\n",
    "\n",
    "`print(f'Precision: {precision:.2f}')`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef98a90",
   "metadata": {},
   "source": [
    "### <font color='coral'><b>Recall - classification models</b></font>\n",
    "\n",
    "- When something is possitive, how often do we predict that it is actually possitive?\n",
    "- key performance metric in machine learning, particularly for classification tasks. It **measures the ability** of a model to identify all relevant instances within a dataset. In other words, recall tells you how many of the actual positive instances were correctly identified by the model.\n",
    "\n",
    "<span style=\"background-color: peachpuff;\">Recall = True Positives/ True Positives + False negatives</span>\n",
    "\n",
    "- High recall = good but it depends on the goal of the project \n",
    "\n",
    "- Low recall = generally bad\n",
    "\n",
    "**Metric** \n",
    "\n",
    "`from sklearn.metrics import recall_score`\n",
    "\n",
    "`# Example: Actual vs. Predicted labels`\n",
    "\n",
    "`y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 1]  # Actual labels`\n",
    "\n",
    "`y_pred = [1, 0, 1, 0, 0, 1, 1, 0, 1, 1]  # Predicted labels`\n",
    "\n",
    "`# Calculate Recall`\n",
    "\n",
    "`recall = recall_score(y_true, y_pred)`\n",
    "\n",
    "`print(f'Recall: {recall:.2f}')`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d3662a",
   "metadata": {},
   "source": [
    "### <font color='coral'><b> F1 Score - classification models </b></font>\n",
    "\n",
    " - Is a metric used to evaluate the performance of a classification model. It is the harmonic mean of precision and recall, providing a balance between the two. The F1 score is particularly useful when you need to balance the trade-offs between precision and recall, especially in cases where you have an uneven class distribution like medical diagnosis data and fraud detection data. In medical diagnosis the sick people are way less than the healthy and fradulent transactions are way less to the ration of non fradulent transactions. \n",
    "\n",
    "- range from 0 -1, 1 is the best possible score meaning you have the best recall and precision\n",
    "\n",
    "\n",
    "<span style=\"background-color: peachpuff;\">F1 score = 2 x precision x recal / precision + recall</span>\n",
    "    \n",
    "Precision: 80% (0.8)\n",
    "\n",
    "Recall: 67% (0.67)\n",
    "\n",
    "F1¬†Score = 2 √ó 0.8 √ó 0.67 / 0.8 + 0.67 = 0.73\n",
    "\n",
    "This score provides a single metric that balances both precision and recall, giving you a better overall measure of your model's performance.\n",
    "\n",
    "**Metric** \n",
    "\n",
    "`from sklearn.metrics import f1_score`\n",
    "\n",
    "`# Example data: actual labels vs. predicted labels`\n",
    "\n",
    "`y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]  # Actual values`\n",
    "\n",
    "`y_pred = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]  # Predicted values`\n",
    "\n",
    "`# Calculate F1-score`\n",
    "\n",
    "`f1 = f1_score(y_true, y_pred)`\n",
    "\n",
    "`print(\"F1 Score:\", f1)`\n",
    "\n",
    "\n",
    "### <font color='coral'><b>Precision- Recall Tradeoff - classification models </b></font>\n",
    "\n",
    "\n",
    "- use when youre dealing with an imbalanced data set. An imbalanced dataset is one where the classes are not represented equally. In other words, one class (often the negative class) has significantly more instances than the other class (often the positive class). This imbalance can pose challenges for machine learning models, as they may become biased towards the majority class and perform poorly on the minority class.\n",
    "\n",
    "- the balance between precision and recall. Improving one often comes at the expense of the other. For example, increasing precision might decrease recall, and vice versa. This tradeoff is important when deciding which metric to prioritize based on the specific application.\n",
    "\n",
    "- Example:\n",
    "Imagine you have a binary classification model that predicts the probability of an email being spam. The model outputs a probability score between 0 and 1 for each email. The threshold determines at what probability value you classify an email as spam.\n",
    "\n",
    "- Threshold = 0.5: If the predicted probability is greater than or equal to 0.5, classify the email as spam; otherwise, classify it as not spam.\n",
    "\n",
    "- Lower Threshold (e.g., 0.3): More emails will be classified as spam, increasing recall but potentially decreasing precision.\n",
    "\n",
    "- Higher Threshold (e.g., 0.7): Fewer emails will be classified as spam, increasing precision but potentially decreasing recall.\n",
    "\n",
    "- Precision-Recall Curve:\n",
    "Axes: Plots Precision on the y-axis and Recall on the x-axis.\n",
    "\n",
    "- Use Case: Particularly useful for imbalanced datasets where the positive class is much less frequent than the negative class.\n",
    "\n",
    "- Interpretation: A curve closer to the top right corner indicates better performance. It helps you understand the trade-off between precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53fbdb7-476f-4b5f-a7ea-1e83040d1673",
   "metadata": {},
   "source": [
    "### <font color='coral'><b>Classification Report - classification models  </b></font>\n",
    "\n",
    "- Provides a detailed breakdown of model performance (precision, recall, F1-score). <span style=\"background-color: peachpuff;\">Helps evaluate imbalanced datasets more effectively than just accuracy.</span>\n",
    " Essential for understanding misclassification rates across different classes. Make sure to run classification report on both training data and test data. Training data will shows how well the model learned patterns and test data will show how well the model generalizes to unseen data. They both should have similar performance on both training and test data.\n",
    "- Helps detect overfitting (if training performance is much higher than test performance). Highlights imbalances in precision, recall, and F1-score between different classes.\n",
    "- If your dataset is balanced, accuracy might already give a clear picture and you wouldnt need classification report. Rule of thumb for myself never ever just use accuracy, keep evaluating the model is always best. \n",
    "\n",
    "**Model**\n",
    "\n",
    "`from sklearn.linear_model import LogisticRegression`\n",
    "\n",
    "`# Instantiate the model`\n",
    "\n",
    "`logreg = LogisticRegression()`\n",
    "\n",
    "`# Fit the model`\n",
    "\n",
    "`logreg.fit(X_train, y_train)`\n",
    "\n",
    "`# Predict probabilities`\n",
    "\n",
    "`y_pred = logreg.predict(X_test)`\n",
    "\n",
    "\n",
    "**Metric**\n",
    "\n",
    "`from sklearn.metrics import classification_report`\n",
    "\n",
    "`# make sure to compare y_train and y_pred for the training data`\n",
    "\n",
    "`report = classification_report(y_train, y_pred)`\n",
    "\n",
    "`# make sure to compare y_test and y_pred for the training data `\n",
    "\n",
    "`report = classification_report(y_test, y_pred)`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b52f782",
   "metadata": {},
   "source": [
    "### <font color='coral'><b>Receiver Operating Characteristics - classification models</b></font>\n",
    "\n",
    "\n",
    "\n",
    "- ROC Curve: is a graph that shows how well a classification model distinguishes between positive and negative cases.\n",
    "Axes: Plots the True Positive Rate (Recall) on the y-axis and the False Positive Rate (1 - Specificity) on the x-axis. Useful when the classes are balanced or when you want to understand the trade-off between true positives and false positives.\n",
    "\n",
    "- Interpretation: A curve closer to the top left corner indicates better performance. The Area Under the Curve (AUC) summarizes the overall performance.\n",
    "\n",
    "\n",
    "**True Positive Rate** = **Recall**\n",
    "\n",
    "TP/TP + FN\n",
    "_________________\n",
    "         \n",
    "**True Negative Rate (TNR)** = **Specificity** \n",
    "\n",
    "- TN/ TN + FP\n",
    "_________________\n",
    "\n",
    "**False possitive rate** \n",
    "1-specificity\n",
    "\n",
    "\n",
    "**Metric** \n",
    "\n",
    "`from sklearn.metrics import roc_curve, auc`\n",
    "\n",
    "`import matplotlib.pyplot as plt`\n",
    "\n",
    "`# Assuming you already have true labels (y_test) and predicted probabilities (y_scores)`\n",
    "\n",
    "`fpr, tpr, _ = roc_curve(y_test, y_scores)`\n",
    "\n",
    "`# Calculate AUC (Area Under the Curve)`\n",
    "\n",
    "`roc_auc = auc(fpr, tpr)`\n",
    "\n",
    "`# Plot the ROC curve`\n",
    "\n",
    "`plt.figure(figsize=(6,6))`\n",
    "\n",
    "`plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc:.2f})')`\n",
    "\n",
    "`plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Random guess line`\n",
    "\n",
    "`plt.xlabel('False Positive Rate')`\n",
    "\n",
    "`plt.ylabel('True Positive Rate')`\n",
    "\n",
    "`plt.title('ROC Curve')`\n",
    "\n",
    "`plt.legend(loc='lower right')`\n",
    "\n",
    "`plt.show()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24f7e75",
   "metadata": {},
   "source": [
    "### <font color='coral'><b>AUC Area Under the Curve - classification models </b></font>\n",
    "\n",
    "- Area Under the Curve, is a single scalar value that summarizes the overall performance of a binary classification model. It is derived from the ROC curve. The AUC (Area Under the Curve) represents the entire area beneath the ROC curve, which plots True Positive Rate (TPR) on the y-axis and False Positive Rate (FPR) on the x-axis\n",
    "\n",
    "AUC Interpretation:\n",
    "AUC = 1: Perfect model with no false positives or false negatives.\n",
    "\n",
    "AUC > 0.9: Excellent model performance.\n",
    "\n",
    "AUC between 0.8 and 0.9: Good model performance.\n",
    "\n",
    "AUC between 0.7 and 0.8: Fair model performance.\n",
    "\n",
    "AUC between 0.6 and 0.7: Poor model performance.\n",
    "\n",
    "AUC = 0.5: Model with no discrimination ability, equivalent to random guessing. \n",
    "\n",
    "**Metric** \n",
    "\n",
    "`from sklearn.metrics import roc_auc_score`\n",
    "\n",
    "`# Assuming you already have true labels (y_test) and predicted probabilities (y_pred)`\n",
    "\n",
    "`auc_score = roc_auc_score(y_test, y_pred)`\n",
    "\n",
    "`print(f\"AUC Score: {auc_score:.2f}\")`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70790ab8-86be-40be-80d8-d36c66bc16b7",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-warning\">\n",
    "<h3>Clustering Model Evaluation Methods</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f719d0d1-f235-4341-bb3f-5c594575e5d4",
   "metadata": {},
   "source": [
    "### <font color='coral'><b>Elbow Method - KMeans </b></font>\n",
    "\n",
    "\n",
    "The Elbow Method is a technique used to determine the optimal number of clusters (k) in K-Means clustering. It helps identify the point where adding more clusters no longer significantly improves the model‚Äôs performance.\n",
    "\n",
    "**How It Works**\n",
    "\n",
    "1Ô∏è‚É£ Inertia ‚Üí Think of it as how well your clusters fit the data. Lower inertia means your points are grouped more tightly and meaningfully.\n",
    "\n",
    "2Ô∏è‚É£ Plot Inertia vs. Number of Clusters ‚Üí You test different numbers of clusters (k) and measure how well they organize the data. Then, you plot the results to see how inertia changes.\n",
    "\n",
    "3Ô∏è‚É£ Find the \"Elbow\" Point ‚Üí Imagine bending your arm‚Äîthere‚Äôs a sharp change where it bends. In the plot, this \"bend\" shows where adding more clusters stops making a big difference in organizing the data. That‚Äôs your best number of clusters!\n",
    "\n",
    "**Metric**\n",
    "\n",
    "`ks = range(1, 6)`\n",
    "\n",
    "`inertias = []`\n",
    "\n",
    "`for k in ks:`\n",
    "\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters = k)\n",
    "    \n",
    "    # Fit model to data\n",
    "    model.fit(data)\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "`# Plot ks vs inertias`\n",
    "\n",
    "`plt.plot(ks, inertias, '-o')`\n",
    "\n",
    "`plt.xlabel('number of clusters, k')`\n",
    "\n",
    "`plt.ylabel('inertia')`\n",
    "\n",
    "`plt.xticks(ks)`\n",
    "\n",
    "`plt.show()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46fed93-91b4-45c4-a306-d198385e35ae",
   "metadata": {},
   "source": [
    "### <font color='coral'><b>Cluster Validity Assessment using External Labels - KMeans </b></font>\n",
    "\n",
    "\n",
    "Cluster validity assessment using external labels is the process of evaluating clustering quality by comparing predicted cluster assignments to actual known categories. This helps measure how well the algorithm groups similar data points relative to predefined labels. The data may need to go trough feature transformation may need to go trough scalling or normalization. You can build a pipeline that does the feature transformation, does kmeans and then build the predictions. \n",
    "\n",
    "**Metric** \n",
    "\n",
    "`# Create a KMeans model with 3 clusters: model`\n",
    "\n",
    "`model = KMeans(n_clusters = 3)`\n",
    "\n",
    "`# Use fit_predict to fit model and obtain cluster labels: labels`\n",
    "\n",
    "`labels = model.fit_predict(data)`\n",
    "\n",
    "`# Create a DataFrame with labels and varieties as columns: df`\n",
    "\n",
    "`df = pd.DataFrame({'labels': labels, 'varieties': varieties})`\n",
    "\n",
    "`# Create crosstab: ct`\n",
    "\n",
    "`ct = pd.crosstab(df['labels'],df['varieties'] )`\n",
    "\n",
    "`# Display ct`\n",
    "\n",
    "`print(ct)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9034e0-291f-4d71-bdd6-caf2266781a7",
   "metadata": {},
   "source": [
    "### <font color='coral'><b>Cluster Validity Assessment using External Labels - Hierarchical Clustering</b></font>\n",
    "\n",
    "- Evaluates how well the clustering results match known categories. It works by comparing the predicted cluster assignments to predefined labels.\n",
    "\n",
    "**Metric**\n",
    "\n",
    "`# Perform the necessary imports`\n",
    "\n",
    "`import pandas as pd`\n",
    "\n",
    "`from scipy.cluster.hierarchy import fcluster`\n",
    "\n",
    "`# Use fcluster to extract labels: labels`\n",
    "\n",
    "`labels = fcluster(mergings,6, criterion='distance')`\n",
    "\n",
    "`# Create a DataFrame with labels and varieties as columns: df`\n",
    "\n",
    "`df = pd.DataFrame({'labels': labels, 'varieties': varieties})`\n",
    "\n",
    "`# Create crosstab: ct`\n",
    "\n",
    "`ct = pd.crosstab(df['labels'], df['varieties'])`\n",
    "\n",
    "`# Display ct`\n",
    "\n",
    "`print(ct)`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0bf2f9-93a3-4a1c-a68f-9c500b1d33e7",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-warning\">\n",
    "<h3>Improving Model Performance with Cross-Validation</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477a06c2",
   "metadata": {},
   "source": [
    "### <font color='coral'><b>Cross Validation - evaluation method for most MLA (unseen data)</b></font>\n",
    "\n",
    "\n",
    "\n",
    "Cross-validation is a technique used in machine learning to evaluate how well a model performs on unseen data. It involves splitting the dataset into multiple parts, training the model on some parts, and testing it on the remaining parts. This process is repeated several times to ensure the model's performance is consistent and reliable. It's like giving your model multiple \"practice tests\" before the final exam to see how well it does in different scenarios.\n",
    "\n",
    "\n",
    "theres different methods to cross validate these are the most common \n",
    "Train-Test Split\n",
    "\n",
    "- K-Fold Cross-Validation\n",
    "\n",
    "- Leave-One-Out Cross-Validation (LOOCV)\n",
    "\n",
    "- Stratified K-Fold Cross-Validation\n",
    "\n",
    "- Repeated K-Fold Cross-Validation\n",
    "\n",
    "- Time Series Cross-Validation\n",
    "\n",
    "**Quick evaluation of a model**\n",
    "\n",
    "\n",
    "`from sklearn.model_selection import cross_val_score`\n",
    "\n",
    "`# Load dataset`\n",
    "\n",
    "`X, y = load_data()  # Replace with actual data loading method`\n",
    "\n",
    "`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)`\n",
    "\n",
    "`model = pick a model()`\n",
    "\n",
    "`cv_scores = cross_val_score(model, X, y, cv=5)`\n",
    "\n",
    "**Custom cross-validation strategie KFold** \n",
    "- need to specify model and evaluation method if no evaluation method given it will defult to using the models default built in scoring method. \n",
    "\n",
    "`# Load dataset`\n",
    "\n",
    "`X, y = load_data()  # Replace with actual data loading method`\n",
    "\n",
    "`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)`\n",
    "\n",
    "`kf = KFold(n_splits=5, shuffle=True, random_state=42)`\n",
    "\n",
    "`model = pick a model() `\n",
    "\n",
    "`# Perform cross-validation`\n",
    "\n",
    "`cv_scores = cross_val_score(model, X, y, cv=kf) # no evaluation method given`\n",
    "\n",
    "**Normal Cross-Validation with Custom Scoring** \n",
    "\n",
    "`from sklearn.model_selection import cross_val_score `\n",
    "\n",
    "`cross_val_score (model, the_input_data, target_variable_trying_to_predict, cv = '#', scoring = 'sccuracy' )`\n",
    "\n",
    "**Cross Validation For Multiple Models** \n",
    "\n",
    "`def cross_validate_model(model, X, y, cv=5):`\n",
    "\n",
    "    \"\"\"Performs cross-validation and returns mean scores for RMSE, MAE, and R2.\"\"\"\n",
    "    \n",
    "    # Cross-validated RMSE (negative values converted to positive)\n",
    "    rmse_scores = cross_val_score(model, X, y, cv=cv, scoring=\"neg_root_mean_squared_error\")\n",
    "    rmse_mean = np.abs(rmse_scores).mean()\n",
    "    \n",
    "    # Cross-validated MAE\n",
    "    mae_scores = cross_val_score(model, X, y, cv=cv, scoring=\"neg_mean_absolute_error\")\n",
    "    mae_mean = np.abs(mae_scores).mean()\n",
    "    \n",
    "    # Cross-validated R2 Score\n",
    "    r2_scores = cross_val_score(model, X, y, cv=cv, scoring=\"r2\")\n",
    "    r2_mean = r2_scores.mean()\n",
    "    \n",
    "    return {\"Cross-Validated RMSE\": rmse_mean, \"Cross-Validated MAE\": mae_mean, \"Cross-Validated R2-SCORE\": r2_mean}\n",
    "\n",
    "`models = {`\n",
    "\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, max_features=None, random_state=0),\n",
    "    \"Support Vector Regressor\": SVR(kernel='rbf', C=1.0, epsilon=0.1),\n",
    "    \"XGBoost Regressor\": XGBRegressor(n_estimators=100)\n",
    "`}`\n",
    "\n",
    "`# Run cross-validation for each model`\n",
    "\n",
    "`cv_results = {name: cross_validate_model(model, X_train, y_train) for name, model in models.items()}`\n",
    "\n",
    "`# Print cross-validation results`\n",
    "\n",
    "`for model_name, metrics in cv_results.items():`\n",
    "\n",
    "    print(f\"{model_name}: {metrics}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811dfff0-8b9e-4a0e-88be-62ebe3b426f8",
   "metadata": {},
   "source": [
    "### Overfitting - too complex \n",
    "\n",
    "The model learns the training data too well, including noise and irrelevant details.\n",
    "\n",
    "It performs very well on the training data but poorly on new, unseen data.\n",
    "\n",
    "Think of it like memorizing answers for an exam instead of understanding the concepts.\n",
    "\n",
    "**Bias-Variance Tradeoff - Overfitting = Low Bias, High Variance**\n",
    "\n",
    "\n",
    "**Signs of Overfitting:**\n",
    "\n",
    "        High accuracy on training data but low accuracy on test data.\n",
    "\n",
    "        The model is too complex with too many features.\n",
    "\n",
    "        Solution: Use regularization (like L1/L2 penalties), simplify the model, or get more training data.\n",
    "\n",
    "### Underfitting - too simple \n",
    "\n",
    "The model is too simple and fails to capture important patterns in the data.\n",
    "\n",
    "It performs poorly on both training and test data.\n",
    "\n",
    "**Bias-Variance Tradeoff - Underfitting = High Bias, Low Variance**\n",
    "\n",
    "\n",
    "**Signs of Underfitting:**\n",
    "\n",
    "        Low accuracy on both training and test data.\n",
    "\n",
    "        Model predictions are too simplistic.\n",
    "\n",
    "        Solution: Increase model complexity, use better features, or try a more advanced algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffed6d3-80f8-4649-84bc-a705b89730d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
