{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06fbb91b-5e82-4db1-bd09-c2cb22afbeea",
   "metadata": {},
   "source": [
    "1. Problem identification \n",
    "\n",
    "2. Data wrangling\n",
    "\n",
    "3. Exploratory data analysis\n",
    "\n",
    "4. Prep-processing and training data development\n",
    "\n",
    "5. **Modeling (Machine learning steps)**\n",
    "\n",
    "6. Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e49d75-2a45-4927-9287-e746a087d7a6",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-warning\">\n",
    "<h3>Clustering Models</h3>\n",
    "\n",
    "- Clustering models are unsupervised machine learning algorithms that group data points into clusters based on similarity, without using labeled outcomes.\n",
    "- Clustering models identify natural groupings or patterns in data by measuring how similar or close data points are to one another. Each cluster contains data points that are more similar to each other than to those in other clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a07d4a-2d95-4ea5-afee-76e00c584b48",
   "metadata": {},
   "source": [
    "### <font color='brown'><b> K-means </b></font> \n",
    "\n",
    "- K-Means is an **unsupervised learning** machine learning algorithm used for clustering data into groups. It works by:\n",
    "\n",
    "1️⃣ Choosing a number of clusters K.\n",
    "\n",
    "2️⃣Assigning data points to the nearest cluster center (centroid).\n",
    "\n",
    "3️⃣Updating centroids based on assigned points.\n",
    "\n",
    "4️⃣ Repeating until centroids stabilize.\n",
    "\n",
    "**<span style=\"background-color: goldenrod;\">Model Code</span>**\n",
    "\n",
    "```python\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a KMeans instance with 3 clusters: model\n",
    "model = KMeans( n_clusters= 3 )\n",
    "\n",
    "# Fit model to points\n",
    "model.fit(array)\n",
    "# Determine the cluster labels of the new array \n",
    "labels = model.predict(new_array)\n",
    "\n",
    "# Print cluster labels of new_array\n",
    "print(labels)\n",
    "\n",
    "#Vizualize result\n",
    "\n",
    "# Import pyplot\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Assign the columns of new_array: xs and ys\n",
    "xs =new_array[:,0]\n",
    "ys = new_array[:,1]\n",
    "\n",
    "# Make a scatter plot of xs and ys, using labels to define the colors\n",
    "plt.scatter(xs, ys,c=labels,  alpha=0.5 )\n",
    "\n",
    "# Assign the cluster centers: centroids\n",
    "centroids = model.cluster_centers_\n",
    "\n",
    "# Assign the columns of centroids: centroids_x, centroids_y\n",
    "centroids_x = centroids[:,0]\n",
    "centroids_y = centroids[:,1]\n",
    "\n",
    "# Make a scatter plot of centroids_x and centroids_y\n",
    "plt.scatter( centroids_x,centroids_y, marker = 'D', s= 50)\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "**<span style=\"background-color: goldenrod;\">Scenerios To Use Model In</span>**\n",
    "\n",
    "**<span style=\"color: yellowgreen;\">Scenario 1</span>**\n",
    "\n",
    "- Company: A healthcare analytics startup\n",
    "\n",
    "- Problem: “Can you group patients into risk profiles based on age, BMI, blood pressure, and lab results?”\n",
    "\n",
    "**Why Clustering** (e.g., K-Means or Hierarchical Clustering):\n",
    "\n",
    "- No labeled outcomes — we’re discovering patterns, not predicting a target\n",
    "\n",
    "- Useful for identifying hidden subgroups (e.g., metabolic syndrome, cardiovascular risk)\n",
    "\n",
    "- Helps clinicians personalize care without needing predefined categories\n",
    "\n",
    "**Metrics**\n",
    "\n",
    "Silhouette Score: This tells you how well each patient fits within their assigned cluster. A high score means patients are well-separated and the clusters are meaningful.\n",
    "\n",
    "Davies-Bouldin Index: Measures how compact and distinct the clusters are. A lower score means better-defined groupings — ideal for clinical interpretation.\n",
    "\n",
    "Visual Inspection (e.g., t-SNE or PCA plots): Helps stakeholders see how patients are grouped. If clusters are clearly separated, it builds trust in the model’s insights.\n",
    "\n",
    "\n",
    "**<span style=\"color: yellowgreen;\">Scenario 2</span>**\n",
    "\n",
    "- Company: An e-commerce platform\n",
    "\n",
    "- Problem: “Can you segment customers based on purchase frequency, average spend, and browsing behavior?”\n",
    "\n",
    "**Why Clustering** (e.g., K-Means or DBSCAN):\n",
    "\n",
    "- No target variable — we’re uncovering behavioral patterns\n",
    "\n",
    "- Enables personalized marketing and product recommendations\n",
    "\n",
    "- Helps identify high-value vs. casual shoppers without manual tagging\n",
    "\n",
    "**Metrics**\n",
    "\n",
    "Silhouette Score: Tells you how well each customer fits into their segment. A high score means your clusters reflect real behavioral differences.\n",
    "\n",
    "Calinski-Harabasz Index: Measures cluster separation and cohesion. A higher score means your segments are distinct and actionable.\n",
    "\n",
    "Cluster Profiling: Once clusters are formed, you analyze their characteristics (e.g., “Cluster A spends $200/month, browses skincare, shops late at night”) — this is what drives business decisions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aeac16-bba1-48c7-b356-bf0d0794f8fc",
   "metadata": {},
   "source": [
    "### <font color='brown'><b>Agglomerative Hierarchical Clustering</b></font> \n",
    "\n",
    "\n",
    "Agglomerative Hierarchical Clustering is a bottom-up clustering method that starts with each data point as its own cluster and gradually merges the closest clusters until all points belong to a single cluster or a predefined number of clusters is reached.\n",
    "\n",
    "**How It Works**\n",
    "\n",
    "1️⃣ Start with individual clusters → Each data point is its own cluster. \n",
    "\n",
    "2️⃣ Merge closest clusters → The algorithm finds the two most similar clusters and combines them but but how \"similarity\" is measured depends on the linkage method you choose.\n",
    "\n",
    "    - Single Linkage → Merges clusters based on the closest points.\n",
    "    - Complete Linkage → Uses the farthest points to decide merging\n",
    "    - Average Linkage → Calculates the average distance between clusters\n",
    "    - Ward’s Method → Minimizes the variation inside clusters.\n",
    "    - Centroid Linkage → Uses the center (mean) of clusters to merge them.\n",
    "\n",
    "        \n",
    "\n",
    "3️⃣ Repeat until desired clusters → This continues until a stopping condition is met (e.g., reaching a set number of clusters). \n",
    "\n",
    "4️⃣ Visualize with a dendrogram → The merging process can be represented as a tree-like diagram called a dendrogram, which helps understand how clusters form.\n",
    "\n",
    "**<span style=\"background-color: goldenrod;\">Model Code</span>**\n",
    "\n",
    "```python\n",
    "# Perform the necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "# Select relevant features for clustering, need to be 2D or more.\n",
    "\n",
    "samples = df[['feature1', 'feature2']].values\n",
    "\n",
    "# Calculate the linkage, determines how clusters are merged based on their distances\n",
    "# theres more methods single, average, wards\n",
    "\n",
    "mergings = linkage(samples, method='complete')\n",
    "\n",
    "#Visualize the dendogram using labels. Note labels are only used to vizualise not train model \n",
    "\n",
    "# load labels\n",
    "varieties = df['labels'].values\n",
    "\n",
    "# Plot the dendrogram, using varieties as labels\n",
    "dendrogram(mergings,\n",
    "           labels= varieties,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=6,)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "**<span style=\"background-color: goldenrod;\">Model Evaluations</span>**\n",
    "\n",
    "Cluster Validity Assessment using External Labels- using external labels is the process of evaluating clustering quality by comparing predicted cluster assignments to actual known categories.\n",
    "\n",
    "Cluster Validity Assessment using External Labels. - validating clustering quality by comparing predicted clusters (labels) to ground-truth categories (varieties).\n",
    "\n",
    "Silhouette Score – Measures how well-separated clusters are; higher scores mean better-defined clusters.\n",
    "\n",
    "Davies-Bouldin Index – Assesses cluster compactness and separation; lower values indicate better clustering.\n",
    "\n",
    "Cluster Visualization – Helps understand separation using scatter plots or PCA projections.\n",
    "\n",
    "Scatter plots or Principal Component Analysis (PCA) can help understand separation.\n",
    "\n",
    "Rand Index : metric used to evaluate the similarity between two clustering results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c869ac7-dcb8-4e26-9531-9185075598a6",
   "metadata": {},
   "source": [
    "### <font color='brown'><b>t-NSE (t-Distributed Stochastic Neighbor Embedding)</b></font> \n",
    "\n",
    "- Doesn't cluster but organizes similar points close together, creating a visual effect of clustering. When plotted, distinct groups appear, but t-SNE doesn’t assign cluster labels explicitly like K-Means or Hierarchical Clustering does. What it actually does is it is a dimensionality reduction technique used to visualize high-dimensional data in 2D or 3D space.\n",
    "\n",
    "\n",
    "**<span style=\"background-color: goldenrod;\">Not a Model but Model</span>**\n",
    "\n",
    "\n",
    "```python\n",
    "# Import TSNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# 10–100+ features → Works well for complex datasets\n",
    "samples = df[['feature1', 'feature2', 'feature3', 'feature4', 'feature5', \n",
    "              'feature6', 'feature7', 'feature8', 'feature9', 'feature10']].values\n",
    "\n",
    "\n",
    "# Create a TSNE instance: model\n",
    "model = TSNE(learning_rate=200)\n",
    "\n",
    "# Apply fit_transform to samples: tsne_features\n",
    "tsne_features = model.fit_transform(samples)\n",
    "\n",
    "# Select the 0th feature: xs\n",
    "xs = tsne_features[:,0]\n",
    "\n",
    "# Select the 1st feature: ys\n",
    "ys = tsne_features[:,1]\n",
    "\n",
    "# Scatter plot, coloring by variety_numbers\n",
    "plt.scatter(xs,ys,c=variety_numbers)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**How it is used in the real world**\n",
    "\n",
    "    ✅ Similarity & Grouping → It shows which companies (data points) have similar movement patterns. \n",
    "    ✅ Hidden Structures → Reveals clusters of companies that behave alike, even if not obvious in raw data. \n",
    "    ✅ Anomalies & Outliers → If a company appears far from others, it might behave differently from the rest. \n",
    "    ✅ Market Trends → Can help identify groups of companies with shared movement dynamics (e.g., stock trends).\n",
    "\n",
    "- Silhouette Score\n",
    "- Davies-Bouldin Index\n",
    "- Cluster Visualization\n",
    "- Rand Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1a3672-d574-4554-8fc3-88de3a98356b",
   "metadata": {},
   "source": [
    "### Clustering Algorithms in Scikit-learn\n",
    "<table border=\"1\">\n",
    "<colgroup>\n",
    "<col width=\"15%\" />\n",
    "<col width=\"16%\" />\n",
    "<col width=\"20%\" />\n",
    "<col width=\"27%\" />\n",
    "<col width=\"22%\" />\n",
    "</colgroup>\n",
    "<thead valign=\"bottom\">\n",
    "<tr><th>Method name</th>\n",
    "<th>Parameters</th>\n",
    "<th>Scalability</th>\n",
    "<th>Use Case</th>\n",
    "<th>Geometry (metric used)</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody valign=\"top\">\n",
    "<tr><td>K-Means</span></a></td>\n",
    "<td>number of clusters</td>\n",
    "<td>Very large<span class=\"pre\">n_samples</span>, medium <span class=\"pre\">n_clusters</span> with\n",
    "MiniBatch code</td>\n",
    "<td>General-purpose, even cluster size, flat geometry, not too many clusters</td>\n",
    "<td>Distances between points</td>\n",
    "</tr>\n",
    "<tr><td>Affinity propagation</td>\n",
    "<td>damping, sample preference</td>\n",
    "<td>Not scalable with n_samples</td>\n",
    "<td>Many clusters, uneven cluster size, non-flat geometry</td>\n",
    "<td>Graph distance (e.g. nearest-neighbor graph)</td>\n",
    "</tr>\n",
    "<tr><td>Mean-shift</td>\n",
    "<td>bandwidth</td>\n",
    "<td>Not scalable with <span class=\"pre\">n_samples</span></td>\n",
    "<td>Many clusters, uneven cluster size, non-flat geometry</td>\n",
    "<td>Distances between points</td>\n",
    "</tr>\n",
    "<tr><td>Spectral clustering</td>\n",
    "<td>number of clusters</td>\n",
    "<td>Medium <span class=\"pre\">n_samples</span>, small <span class=\"pre\">n_clusters</span></td>\n",
    "<td>Few clusters, even cluster size, non-flat geometry</td>\n",
    "<td>Graph distance (e.g. nearest-neighbor graph)</td>\n",
    "</tr>\n",
    "<tr><td>Ward hierarchical clustering</td>\n",
    "<td>number of clusters</td>\n",
    "<td>Large <span class=\"pre\">n_samples</span> and <span class=\"pre\">n_clusters</span></td>\n",
    "<td>Many clusters, possibly connectivity constraints</td>\n",
    "<td>Distances between points</td>\n",
    "</tr>\n",
    "<tr><td>Agglomerative clustering</td>\n",
    "<td>number of clusters, linkage type, distance</td>\n",
    "<td>Large <span class=\"pre\">n_samples</span> and <span class=\"pre\">n_clusters</span></td>\n",
    "<td>Many clusters, possibly connectivity constraints, non Euclidean\n",
    "distances</td>\n",
    "<td>Any pairwise distance</td>\n",
    "</tr>\n",
    "<tr><td>DBSCAN</td>\n",
    "<td>neighborhood size</td>\n",
    "<td>Very large <span class=\"pre\">n_samples</span>, medium <span class=\"pre\">n_clusters</span></td>\n",
    "<td>Non-flat geometry, uneven cluster sizes</td>\n",
    "<td>Distances between nearest points</td>\n",
    "</tr>\n",
    "<tr><td>Gaussian mixtures</td>\n",
    "<td>many</td>\n",
    "<td>Not scalable</td>\n",
    "<td>Flat geometry, good for density estimation</td>\n",
    "<td>Mahalanobis distances to  centers</td>\n",
    "</tr>\n",
    "<tr><td>Birch</td>\n",
    "<td>branching factor, threshold, optional global clusterer.</td>\n",
    "<td>Large <span class=\"pre\">n_clusters</span> and <span class=\"pre\">n_samples</span></td>\n",
    "<td>Large dataset, outlier removal, data reduction.</td>\n",
    "<td>Euclidean distance between points</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "Source: http://scikit-learn.org/stable/modules/clustering.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455d35ad-c02a-4b86-b6d1-6ef8e94ca5df",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-warning\">\n",
    "<h3>Clustering Models Evaluation Metrics</h3>\n",
    "\n",
    "Elbow Method – Determines the optimal number of clusters by plotting within-cluster sum of squares (WCSS).\n",
    "\n",
    "Cluster Validity Assessment using External Labels. - validating clustering quality by comparing predicted clusters (labels) to ground-truth categories (varieties).\n",
    "\n",
    "Silhouette Score – Measures how well-separated clusters are; higher scores mean better-defined clusters.\n",
    "\n",
    "Davies-Bouldin Index – Assesses cluster compactness and separation; lower values indicate better clustering.\n",
    "\n",
    "Gap Statistic – Compares clustering results against random data to test effectiveness.\n",
    "\n",
    "Cluster Visualization – Helps understand separation using scatter plots or PCA projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e734524-093a-456c-83ba-5f7355119a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
