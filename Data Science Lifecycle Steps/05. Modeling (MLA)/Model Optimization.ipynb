{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03cbe661-6104-40f3-989b-7ef14ad2cc29",
   "metadata": {},
   "source": [
    "1. Problem identification \n",
    "\n",
    "2. Data wrangling\n",
    "\n",
    "3. Exploratory data analysis\n",
    "\n",
    "4. Prep-processing and training data development\n",
    "\n",
    "5. **Modeling (Machine learning steps)**\n",
    "\n",
    "6. Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75854bfc-1215-4101-8e06-b82742386fb5",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-success\">\n",
    "<h3>Hyperparameter Tunning for MLA</h3>\n",
    "- Can be used on both regression and classification problems.\n",
    "https://www.kdnuggets.com/2022/10/hyperparameter-tuning-grid-search-random-search-python.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce5452b-6556-4d66-8d7b-2990744e20dc",
   "metadata": {},
   "source": [
    "## <font color='purple'><b>GridSearchCV</b></font> \n",
    "\n",
    "Method used in machine learning to find the best hyperparameters for a model by exhaustively searching through a predefined set of possible values. <span style=\"background-color:PowderBlue;\">Every possible combination is tested systematically.</span>\n",
    "\n",
    "  \n",
    "1Ô∏è‚É£ Define the Parameter Grid: You specify a set of possible values for each hyperparameter you want to tune. This creates a grid of all possible combinations of these values.\n",
    "\n",
    "2Ô∏è‚É£ Train and Evaluate: The grid search algorithm trains and evaluates the model for each combination of hyperparameters in the grid. This is typically done using cross-validation to ensure that the evaluation is reliable.\n",
    "\n",
    "3Ô∏è‚É£ Select the Best Combination: After evaluating all combinations, the grid search algorithm selects the combination of hyperparameters that resulted in the best performance according to a specified metric (e.g., accuracy, RMSE).\n",
    "\n",
    "\n",
    "Model examples and their parameters \n",
    "- KNeighborsClassifier(n_neighbors= )\n",
    "  \n",
    "        - n_neighbors \n",
    "        - weights \n",
    "        - metric \n",
    "        - p\n",
    "\n",
    "- Decision Tree\n",
    "\n",
    "        - max_depth ‚Üí Maximum depth of the tree.\n",
    "        - min_samples_split ‚Üí Minimum samples required to split a node.\n",
    "        - criterion ‚Üí Measure of impurity (\"gini\", \"entropy\").\n",
    "        - min_samples_leaf ‚Üí Minimum samples required at a leaf node.\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "**Code - Decision Tree** \n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the Hyperparameter Grid\n",
    "\n",
    "param_grid = {\n",
    "\n",
    "    'n_estimators': [10, 50, 100],  # Example for RandomForest\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "#Set Up GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "#Fit GridSearchCV\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "Retrieve the Best Parameters\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Analyze Results\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "print(cv_results[['param_n_estimators', 'param_max_depth', 'mean_test_score']])\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1a99f6-050c-4b32-8877-44ab568b87ab",
   "metadata": {},
   "source": [
    "## <font color='purple'><b>RandomisedSearchCV</b></font> \n",
    "\n",
    "\n",
    "RandomizedSearchCV is a hyperparameter tuning method in scikit-learn that searches for the best combination of hyperparameters by <span style=\"background-color: PowderBlue;\">randomly sampling from a predefined distribution instead of exhaustively testing all possible values</span> like GridSearchCV does.\n",
    "\n",
    "**Code** \n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define model\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameter distribution\n",
    "\n",
    "param_dist = {\n",
    "\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "    \n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "\n",
    "random_search = RandomizedSearchCV(model, param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best parameters\n",
    "\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce58d02-a5bb-4b52-bee0-d42e0e36474c",
   "metadata": {},
   "source": [
    "## <font color='purple'><b>Bayesian Optimization</b></font> \n",
    "\n",
    "Bayesian Optimization is a smart way to find the best hyperparameters for a machine learning model without testing every possible option. It does this by doing <span style=\"background-color: PowderBlue;\">balancing exploration where it tries a few random values, learns from their results, and then balances exploitation where it uses what it learned in exploration to make better guesses.</span> While this method dynamically adjusts based on prior results, it still operates within a predefined search space, making optimization faster and more efficient than traditional methods like grid search.\n",
    "\n",
    "Steps \n",
    "\n",
    "1Ô∏è‚É£ Define the function ‚Äì Create a function that takes in parameters and outputs a single number, which represents how good those parameters are.\n",
    "\n",
    "2Ô∏è‚É£ Set up Bayesian Optimization ‚Äì Specify the function to optimize and define the range of possible values for each parameter.\n",
    "\n",
    "3Ô∏è‚É£ Run the optimization ‚Äì Start with random exploration (init_points) and then let the optimizer make smarter guesses (n_iter).\n",
    "\n",
    "4Ô∏è‚É£ Retrieve the best parameters ‚Äì At the end, check which values gave the best result for optimization.\n",
    "\n",
    "\n",
    "**Code**\n",
    "\n",
    "```python\n",
    "from bayesian_optimization import BayesianOptimization\n",
    "\n",
    "# 1. Define the function you want to optimize\n",
    "\n",
    "def objective_function(a, b):\n",
    "\n",
    "    return a + b  # Example: Maximizing the sum of two parameters\n",
    "\n",
    "# 2. Set up the Bayesian Optimizer with predefined search space\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "\n",
    "    f=objective_function,\n",
    "    pbounds={'a': (1, 3), 'b': (4, 7)},\n",
    "    random_state=42  # For reproducibility\n",
    "    \n",
    ")\n",
    "\n",
    "# 3. Run optimization\n",
    "\n",
    "optimizer.maximize(`   # always use .maximize()\n",
    "\n",
    "    init_points=3,  # Number of random explorations\n",
    "    n_iter=5        # Number of optimization iterations\n",
    "    \n",
    ")\n",
    "\n",
    "# 4. Print the best parameters and their associated maximized target\n",
    "\n",
    "print(\"Best Parameters:\", optimizer.max['params'])\n",
    "\n",
    "print(\"Best Target Value:\", optimizer.max['target'])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c5b72c-46d8-4151-b27d-d62499745300",
   "metadata": {},
   "source": [
    "## <font color='purple'><b>Cross Validation</b></font> \n",
    "\n",
    "Method used in machine learning to evaluate a model‚Äôs performance by splitting the data into multiple parts and testing it on different subsets. <span style=\"background-color:PowderBlue;\">It helps ensure your model generalizes well to unseen data.</span>\n",
    "\n",
    "üîÅ How It Works\n",
    "1Ô∏è‚É£ Split the Data: The dataset is divided into k equal parts (called ‚Äúfolds‚Äù). 2Ô∏è‚É£ Train and Test: The model is trained on k‚Äì1 folds and tested on the remaining fold. This process repeats k times, each time using a different fold for testing. 3Ô∏è‚É£ Average the Scores: The performance scores from each fold are averaged to give a more reliable estimate of how the model will perform in the real world.\n",
    "\n",
    "Why Use Cross-Validation\n",
    "Reduces the risk of overfitting\n",
    "\n",
    "Gives a more accurate picture of model performance\n",
    "\n",
    "Works with both regression and classification models\n",
    "\n",
    "Often used inside hyperparameter tuning methods like GridSearchCV\n",
    "\n",
    "\n",
    "Model Examples\n",
    "LogisticRegression()\n",
    "\n",
    "penalty\n",
    "\n",
    "C\n",
    "\n",
    "solver\n",
    "\n",
    "RandomForestClassifier()\n",
    "\n",
    "n_estimators\n",
    "\n",
    "max_depth\n",
    "\n",
    "min_samples_split\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Apply cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print results\n",
    "print(\"Cross-Validation Scores:\", scores)\n",
    "print(\"Mean Accuracy:\", scores.mean())\n",
    "\n",
    "#optional \n",
    "# For precision\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='precision')\n",
    "\n",
    "# For recall\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='recall')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8edfc3-14c8-4809-93e4-2566066b67bd",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-success\">\n",
    "<h3>Hyperparameter Tunning for Deep Learning</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637df997-5581-4540-b9aa-0ca32ad292ab",
   "metadata": {},
   "source": [
    "## <font color='purple'><b>KerasClassifier</b></font> \n",
    "\n",
    "- When using KerasClassifier (or KerasRegressor), your model must be wrapped inside a Python function. That‚Äôs because GridSearchCV or RandomizedSearchCV from scikit-learn needs to be able to rebuild the model fresh for each combination of hyperparameters it tries.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
